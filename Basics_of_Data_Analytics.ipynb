{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Basics of Data Analytics"
      ],
      "metadata": {
        "id": "io6dQ6avd_En"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the role of a Data Analyst in your own words. What value do they bring to an organization?\n",
        "   -A data analyst is responsible for gathering, cleaning, organizing, and analyzing data to extract insights that support better decision-making within an organization. Their work typically involves using statistics, specialized tools, and visualization techniques to turn complex datasets into actionable reports and recommendations for stakeholders. The Following value they brought to an Organization:-\n",
        "   a. Make informed, data-driven decisions that improve efficiency, optimize operations, and enhance profitability.\n",
        "\n",
        "   b. Identify business opportunities and risks through analysis of trends, customer behavior and operational performance.\n",
        "\n",
        "   c. Communicate findings through clear, visual, and easy-to-understand reports and dashboards, making complex data accessible to non-technical.\n",
        "\n",
        "   d. Improve overall data quality and support regulatory compliance by ensuring data integrity, accuracy, and security.\n",
        "\n",
        "   e. Collaborate across multiple departments like marketing, finance, operation and HR, providing tailored analytical support for different business needs.\n",
        "\n",
        "2. List three tools commonly used by Data Analysts and explain what each is mainly used for.\n",
        "   -Three tools commonly used by data analysts are Python, Microsoft Excel, and Power BI.\n",
        "   a. Microsoft Excel:- It remains one of the most popular tools for data analysts, especially for data manipulation, quick calculations, and creating summary reports with features like pivot tables and Power Query. It is often the first tool used for exploring new datasets and building straightforward visualizations and dashboards.\n",
        "   b. Python:- Python is a leading programming language for data analysts due to its versatile libraries such as Pandas, NumPy, and Matplotlib. It is mainly used for automating data cleaning, performing statistical analysis, building predictive models, and creating custom visualizations.\n",
        "   c. Power BI:- Power BI, developed by Microsoft, is widely used for interactive data visualization and business intelligence reporting. It allows analysts to transform complex data into clear, interactive dashboards and reports, enabling organizations to monitor performance and discover trends in real time.\n",
        "\n",
        "3. Write the end-to-end analytics workflow in the correct order and briefly explain each step.\n",
        "\n",
        "  - An end-to-end analytic workflow involves a clear, ordered set of steps that take you from a business question to actionable insights and decisions. Here the typical sequence with a preview explanation for each step:\n",
        "  a. Define the Problem or Question\n",
        "  Begin by identifying the core issue, business opportunity, or question the analysis should address. This step shapes the objectives and ensures the work stays focused and relevant to stakeholders.\n",
        "\n",
        "  b. Data Collection\n",
        "  Gather necessary data from available sources such as databases, APIs, surveys, or external files. Data must be relevant, sufficient, and well-documented to ensure reliable results.\n",
        "\n",
        "  c. Data Cleaning and Preparation\n",
        "  Prepare data for analysis by handling missing values, fixing errors, and transforming it into a suitable format. This step is crucial for ensuring the accuracy and integrity of further analysis.\n",
        "\n",
        "  d. Data Analysis and Exploration\n",
        "  Apply statistical analysis, machine learning models, or other techniques to discover patterns, trends, and insights. Exploratory data analysis often precedes deeper modeling to uncover relationships and guide subsequent steps.\n",
        "\n",
        "  e. Interpretation & Reporting\n",
        "  Interpret your findings, relate them to the original question, and translate them into practical recommendations. Share results through dashboards, charts, and written reports tailored for clear communication to stakeholders.\n",
        "\n",
        "  f. Decision-Making & Action\n",
        "  Use the analysis and recommendations to make informed business decisions, implement changes, or support further research. Feedback from this step may prompt new analyses, creating a continuous improvement cycle. Each step builds on the previous one to ensure the integrity relevance, and impact of analytical work in any organization.\n",
        "\n",
        "4. What is Prompt Engineering, and explain any two types of prompts used in Generative AI?\n",
        "  -Prompt engineering is the process of designing, refining and structuring inputs (\"prompts\") to guide generative AI systems like large language models to produce specific, high-quality outputs. Two Types of Prompts in Generative AI a. Instruction Prompts Instruction prompts directly tell the AI what task to perform. For example: “Write a summary of this news article” or “Create a Python script to sort a list.” These prompts are ideal for straightforward tasks, giving the model a clear directive so it follows specific instructions. b. Contextual or Few-Shot Prompts Contextual or few-shot prompts provide the model with background information or examples to guide its response. For example: “Here are two examples of friendly replies to customer queries. Write a similar response for this new query.” This method is particularly useful for complex tasks or when a nuanced answer is needed, as the AI can mimic provided patterns or styles. Both types are fundamental to getting valuable results from generative AI, whether for crafting answers, generating code, or producing creative content.\n",
        "\n",
        "5. What are the key differences between a Business Analyst and a Data Analyst in terms of roles, responsibilities, and focus?\n",
        "\n",
        "  -Role and Focus Business Analyst (BA): Focuses on understanding business needs, identifying problems or opportunities, and recommending solutions that align with business goals. BAs act as intermediaries between stakeholders and technical teams to translate business requirements into actionable plans.\n",
        "  Data Analyst (DA): Concentrates on collecting, cleaning, and analyzing complex datasets to identify trends, patterns, and insights that support decision-making. DAs dive deep into data itself, using statistical and technical skills to generate meaningful reports and visualizations.\n",
        "\n",
        "  Responsibilities Business Analyst: a. Gathering and documenting business requirements. b. Conducting feasibility and risk assessments. c. Collaborating with stakeholders to improve processes and workflows. d. Supporting project management and ensuring alignment of IT and business strategies. e. Presenting findings and facilitating communication among teams.\n",
        "\n",
        "  Data Analyst: a. Collecting data from various sources. b. Cleaning and preparing data for analysis. c. Performing exploratory data analysis and predictive modeling. d. Creating data visualizations and reports to communicate insights. e. Using statistical techniques and programming (e.g., SQL, Python) to analyze data.\n",
        "\n",
        "6. Explain any three AI-powered ETL (Extract, Transform, Load) tools and how they are used in analytics.\n",
        "   -three AI-powered ETL (Extract, Transform, Load) tools and how they are used in analytics:\n",
        "   Talend Data Fabric: Talend integrates AI and machine learning capabilities throughout its data integration platform. In ETL, AI can be used for automated data quality checks, intelligent data matching, and schema inference. For analytics, this means cleaner, more reliable data delivered faster, enabling more accurate insights from business intelligence dashboards and predictive models.\n",
        "\n",
        "   Google Cloud Dataflow: Dataflow is a fully managed service for executing Apache Beam pipelines. While not strictly an 'AI tool' itself, it can leverage AI/ML services within the Google Cloud ecosystem, such as Cloud AI Platform and TensorFlow Extended (TFX). AI-powered transformations can be built into Dataflow pipelines to perform tasks like sentiment analysis on text data, image recognition on visual data, or anomaly detection on time-series data as part of the ETL process. This enriches data before it reaches analytical databases, providing deeper, AI-derived features for downstream analytics.\n",
        "\n",
        "   Informatica Intelligent Data Platform (IDP): Informatica uses its 'CLAIRE' AI engine to automate and intelligentize various aspects of data management, including ETL. CLAIRE assists with metadata management, data cataloging, data quality, and data governance. In ETL, this translates to AI-driven recommendations for data transformations, automated discovery of sensitive data, and proactive data quality issue resolution. For analytics, this ensures that analysts are working with highly curated, secure, and contextually rich data, leading to more trustworthy and impactful analytical results.\n",
        "\n",
        "7. Give three applications of Data Analytics across different industries and explain how it creates value.\n",
        "   -a. Retail and E-commerce: Personalized Customer Experiences Application: Data analytics is used to analyze customer browsing history, purchase patterns, demographics, and even social media interactions. This allows retailers to create highly personalized recommendations, targeted promotions, and customized shopping experiences.\n",
        "   Value Creation: By understanding individual customer preferences, businesses can increase sales through upselling and cross-selling, improve customer loyalty, reduce marketing waste by targeting relevant audiences, and enhance the overall customer journey, leading to higher customer satisfaction and repeat business.\n",
        "   b. Healthcare: Predictive Analytics for Patient Outcomes\n",
        "   Application: Healthcare organizations leverage data analytics to analyze vast amounts of patient data, including electronic health records (EHRs), diagnostic images, genetic information, and real-time sensor data. This enables the prediction of disease outbreaks, identification of at-risk patients for certain conditions, optimization of treatment plans, and monitoring of patient health post-discharge. Value Creation: This leads to proactive interventions, potentially saving lives and reducing healthcare costs by preventing serious complications. It also helps in resource allocation (e.g., staffing, bed management), drug discovery, and improving the efficiency and effectiveness of healthcare delivery.\n",
        "   c. Finance and Banking: Fraud Detection and Risk Management Application: Financial institutions employ data analytics, particularly machine learning algorithms, to detect fraudulent transactions in real-time. By analyzing patterns in spending habits, transaction locations, amounts, and frequency, unusual activities can be flagged instantly. Additionally, analytics is used for credit scoring, assessing loan risks, and predicting market trends.\n",
        "   Value Creation: Early detection of fraud saves banks and customers billions of dollars annually. It also protects the reputation of financial institutions and reduces operational losses. In risk management, it enables more accurate lending decisions, minimizes defaults, and helps navigate volatile markets, thereby safeguarding assets and ensuring financial stability.\n",
        "\n",
        "8. Explain the evolution of analytics, highlighting the different stages from Descriptive to Generative AI.\n",
        "   -The evolution of analytics can be broadly categorized into several stages, each building upon the previous one and offering increasingly sophisticated insights and capabilities:\n",
        "   Descriptive Analytics (What happened?): Focus: This is the most basic form of analytics, aimed at summarizing past events and understanding what has already occurred. It involves data aggregation and data mining to provide insights into historical performance. Tools/Techniques: Reports, dashboards, basic statistics (mean, median, mode), data visualization. Value: Helps organizations understand their current state, identify trends, and gain a foundational understanding of their operations (e.g., \"What were our sales last quarter?\").\n",
        "\n",
        "   Diagnostic Analytics (Why did it happen?): Focus: Moves beyond just knowing what happened to understanding why it happened. It involves more complex techniques to drill down into data, identify anomalies, and uncover root causes. Tools/Techniques: Data discovery, drill-down capabilities, correlation, probability, regression analysis. Value: Provides deeper insights into past events, helping businesses identify factors contributing to successes or failures (e.g., \"Why did sales drop in a particular region last quarter?\").\n",
        "\n",
        "   Predictive Analytics (What will happen?): Focus: Uses historical data to make predictions about future outcomes. It involves statistical models and machine learning algorithms to forecast trends and probabilities. Tools/Techniques: Machine learning (regression, classification), forecasting models, data mining, statistical modeling. Value: Enables organizations to anticipate future events, assess risks, and identify opportunities (e.g., \"What will our sales be next quarter? Which customers are likely to churn?\").\n",
        "\n",
        "   Prescriptive Analytics (What should we do?): Focus: Builds on predictive analytics by recommending specific actions to achieve desired outcomes or mitigate risks. It provides actionable advice. Tools/Techniques: Optimization, simulation, decision trees, graph analysis, recommendation engines. Value: Guides decision-making by suggesting the best course of action given various scenarios and constraints (e.g., \"What marketing strategy should we implement to maximize sales next quarter? What is the optimal pricing for our product?\").\n",
        "\n",
        "   Generative AI (What can be created or designed?): Focus: This is a more recent and advanced stage, where AI models are used not just to analyze or predict, but to create new content, designs, solutions, or even code. It's about generating novel outputs based on learned patterns from vast datasets. Tools/Techniques: Large Language Models (LLMs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), transformers. Value: Revolutionizes creative processes, automates content generation, accelerates research and development, and enables personalized experiences at an unprecedented scale (e.g., \"Generate a marketing campaign for a new product, write code to automate a task, design new drug molecules, create realistic images from text descriptions.\") Each stage represents a progression in the sophistication of data utilization, moving from understanding the past to influencing the future and ultimately creating it."
      ],
      "metadata": {
        "id": "PQCMhNvJ3oQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define the role of a Data Analyst in your own words. What value do they\n",
        "bring to an organization?\n",
        "- A data analyst is responsible for gathering, cleaning, organizing, and analyzing data to extract insights that support better decision-making within an organization. Their work typically involves using statistics, specialized tools, and visualization techniques to turn complex datasets into actionable reports and recommendations for stakeholders.\n",
        "  The Following value they brought to an Organization:-\n",
        "  a. Make informed, data-driven decisions that improve efficiency, optimize operations, and enhance profitability.\n",
        "\n",
        "  b. Identify business opportunities and risks through analysis of trends, customer behavior and operational performance.\n",
        "\n",
        "  c. Communicate findings through clear, visual, and easy-to-understand reports and dashboards, making complex data accessible to non-technical.\n",
        "\n",
        "  d. Improve overall data quality and support regulatory compliance by ensuring data integrity, accuracy, and security.\n",
        "\n",
        "  e. Collaborate across multiple departments like marketing, finance, operation and HR, providing tailored analytical support for different business needs.\n",
        "\n",
        "2. List three tools commonly used by Data Analysts and explain what each is\n",
        "mainly used for.\n",
        "- Three tools commonly used by data analysts are Python, Microsoft Excel, and Power BI.\n",
        "  1. Microsoft Excel:- It remains one of the most popular tools for data analysts, especially for data manipulation, quick calculations, and creating summary reports with features like pivot tables and Power Query. It is often the first tool used for exploring new datasets and building straightforward visualizations and dashboards.\n",
        "  2. Python:- Python is a leading programming language for data analysts due to its versatile libraries such as Pandas, NumPy, and Matplotlib. It is mainly used for automating data cleaning, performing statistical analysis, building predictive models, and creating custom visualizations.\n",
        "  3. Power BI:- Power BI, developed by Microsoft, is widely used for interactive data visualization and business intelligence reporting. It allows analysts to transform complex data into clear, interactive dashboards and reports, enabling organizations to monitor performance and discover trends in real time.\n",
        "\n",
        "3. Write the end-to-end analytics workflow in the correct order and briefly\n",
        "explain each step.\n",
        "   - An end-to-end analytic workflow involves a clear, ordered set of steps that take you from a business question to actionable insights and decisions. Here the typical sequence with a preview explanation for each step:\n",
        "\n",
        "   a. Define the Problem or Question\n",
        "Begin by identifying the core issue, business opportunity, or question the analysis should address. This step shapes the objectives and ensures the work stays focused and relevant to stakeholders.\n",
        "\n",
        "   b. Data Collection\n",
        "Gather necessary data from available sources such as databases, APIs, surveys, or external files. Data must be relevant, sufficient, and well-documented to ensure reliable results.\n",
        "\n",
        "   c. Data Cleaning and Preparation\n",
        "Prepare data for analysis by handling missing values, fixing errors, and transforming it into a suitable format. This step is crucial for ensuring the accuracy and integrity of further analysis.\n",
        "\n",
        "   d. Data Analysis and Exploration\n",
        "Apply statistical analysis, machine learning models, or other techniques to discover patterns, trends, and insights. Exploratory data analysis often precedes deeper modeling to uncover relationships and guide subsequent steps.\n",
        "\n",
        "   e. Interpretation & Reporting\n",
        "Interpret your findings, relate them to the original question, and translate them into practical recommendations. Share results through dashboards, charts, and written reports tailored for clear communication to stakeholders.\n",
        "\n",
        "  f. Decision-Making & Action\n",
        "Use the analysis and recommendations to make informed business decisions, implement changes, or support further research. Feedback from this step may prompt new analyses, creating a continuous improvement cycle.\n",
        "     Each step builds on the previous one to ensure the integrity relevance, and impact of analytical work in any organization.\n",
        "\n",
        "4. What is Prompt Engineering, and explain any two types of prompts used\n",
        "in Generative AI?\n",
        "     -Prompt engineering is the process of designing, refining and structuring inputs (\"prompts\") to guide generative AI systems like large language models to produce specific, high-quality outputs.\n",
        "     Two Types of Prompts in Generative AI\n",
        "     a. Instruction Prompts\n",
        "Instruction prompts directly tell the AI what task to perform. For example: “Write a summary of this news article” or “Create a Python script to sort a list.” These prompts are ideal for straightforward tasks, giving the model a clear directive so it follows specific instructions.\n",
        "     b. Contextual or Few-Shot Prompts\n",
        "Contextual or few-shot prompts provide the model with background information or examples to guide its response. For example: “Here are two examples of friendly replies to customer queries. Write a similar response for this new query.” This method is particularly useful for complex tasks or when a nuanced answer is needed, as the AI can mimic provided patterns or styles.\n",
        "Both types are fundamental to getting valuable results from generative AI, whether for crafting answers, generating code, or producing creative content.\n",
        "\n",
        "5. What are the key differences between a Business Analyst and a Data\n",
        "Analyst in terms of roles, responsibilities, and focus?\n",
        "   - Role and Focus\n",
        "Business Analyst (BA): Focuses on understanding business needs, identifying problems or opportunities, and recommending solutions that align with business goals. BAs act as intermediaries between stakeholders and technical teams to translate business requirements into actionable plans.\n",
        "\n",
        "Data Analyst (DA): Concentrates on collecting, cleaning, and analyzing complex datasets to identify trends, patterns, and insights that support decision-making. DAs dive deep into data itself, using statistical and technical skills to generate meaningful reports and visualizations.\n",
        "\n",
        "Responsibilities\n",
        "Business Analyst:\n",
        "a. Gathering and documenting business requirements.\n",
        "b. Conducting feasibility and risk assessments.\n",
        "c. Collaborating with stakeholders to improve processes and workflows.\n",
        "d. Supporting project management and ensuring alignment of IT and business strategies.\n",
        "e. Presenting findings and facilitating communication among teams.\n",
        "\n",
        "Data Analyst:\n",
        "a. Collecting data from various sources.\n",
        "b. Cleaning and preparing data for analysis.\n",
        "c. Performing exploratory data analysis and predictive modeling.\n",
        "d. Creating data visualizations and reports to communicate insights.\n",
        "e. Using statistical techniques and programming (e.g., SQL, Python) to analyze data.\n",
        "\n",
        "6. Explain any three AI-powered ETL (Extract, Transform, Load) tools and how they are used in analytics.\n",
        "   - three AI-powered ETL (Extract, Transform, Load) tools and how they are used in analytics:\n",
        "\n",
        "Talend Data Fabric: Talend integrates AI and machine learning capabilities throughout its data integration platform. In ETL, AI can be used for automated data quality checks, intelligent data matching, and schema inference. For analytics, this means cleaner, more reliable data delivered faster, enabling more accurate insights from business intelligence dashboards and predictive models.\n",
        "\n",
        "Google Cloud Dataflow: Dataflow is a fully managed service for executing Apache Beam pipelines. While not strictly an 'AI tool' itself, it can leverage AI/ML services within the Google Cloud ecosystem, such as Cloud AI Platform and TensorFlow Extended (TFX). AI-powered transformations can be built into Dataflow pipelines to perform tasks like sentiment analysis on text data, image recognition on visual data, or anomaly detection on time-series data as part of the ETL process. This enriches data before it reaches analytical databases, providing deeper, AI-derived features for downstream analytics.\n",
        "\n",
        "Informatica Intelligent Data Platform (IDP): Informatica uses its 'CLAIRE' AI engine to automate and intelligentize various aspects of data management, including ETL. CLAIRE assists with metadata management, data cataloging, data quality, and data governance. In ETL, this translates to AI-driven recommendations for data transformations, automated discovery of sensitive data, and proactive data quality issue resolution. For analytics, this ensures that analysts are working with highly curated, secure, and contextually rich data, leading to more trustworthy and impactful analytical results.\n",
        "\n",
        "7. Give three applications of Data Analytics across different industries and explain how it creates value.\n",
        "   a. Retail and E-commerce: Personalized Customer Experiences    Application: Data analytics is used to analyze customer browsing history, purchase patterns, demographics, and even social media interactions. This allows retailers to create highly personalized recommendations, targeted promotions, and customized shopping experiences.\n",
        "   Value Creation: By understanding individual customer preferences, businesses can increase sales through upselling and cross-selling, improve customer loyalty, reduce marketing waste by targeting relevant audiences, and enhance the overall customer journey, leading to higher customer satisfaction and repeat business.\n",
        "  b. Healthcare: Predictive Analytics for Patient Outcomes\n",
        "  Application: Healthcare organizations leverage data analytics to analyze vast amounts of patient data, including electronic health records (EHRs), diagnostic images, genetic information, and real-time sensor data. This enables the prediction of disease outbreaks, identification of at-risk patients for certain conditions, optimization of treatment plans, and monitoring of patient health post-discharge.\n",
        "  Value Creation: This leads to proactive interventions, potentially saving lives and reducing healthcare costs by preventing serious complications. It also helps in resource allocation (e.g., staffing, bed management), drug discovery, and improving the efficiency and effectiveness of healthcare delivery.\n",
        "  c. Finance and Banking: Fraud Detection and Risk Management\n",
        "  Application: Financial institutions employ data analytics, particularly machine learning algorithms, to detect fraudulent transactions in real-time. By analyzing patterns in spending habits, transaction locations, amounts, and frequency, unusual activities can be flagged instantly. Additionally, analytics is used for credit scoring, assessing loan risks, and predicting market trends.\n",
        "  Value Creation: Early detection of fraud saves banks and customers billions of dollars annually. It also protects the reputation of financial institutions and reduces operational losses. In risk management, it enables more accurate lending decisions, minimizes defaults, and helps navigate volatile markets, thereby safeguarding assets and ensuring financial stability.  \n",
        "\n",
        "8. Explain the evolution of analytics, highlighting the different stages from Descriptive to Generative AI.\n",
        "   - The evolution of analytics can be broadly categorized into several stages, each building upon the previous one and offering increasingly sophisticated insights and capabilities:\n",
        "\n",
        "  Descriptive Analytics (What happened?):\n",
        "  Focus: This is the most basic form of analytics, aimed at summarizing past events and understanding what has already occurred. It involves data aggregation and data mining to provide insights into historical performance.\n",
        "  Tools/Techniques: Reports, dashboards, basic statistics (mean, median, mode), data visualization.\n",
        "  Value: Helps organizations understand their current state, identify trends, and gain a foundational understanding of their operations (e.g., \"What were our sales last quarter?\").\n",
        "  \n",
        "  Diagnostic Analytics (Why did it happen?):\n",
        "  Focus: Moves beyond just knowing what happened to understanding why it happened. It involves more complex techniques to drill down into data, identify anomalies, and uncover root causes.\n",
        "  Tools/Techniques: Data discovery, drill-down capabilities, correlation, probability, regression analysis.\n",
        "  Value: Provides deeper insights into past events, helping businesses identify factors contributing to successes or failures (e.g., \"Why did sales drop in a particular region last quarter?\").\n",
        "  \n",
        "  Predictive Analytics (What will happen?):\n",
        "  Focus: Uses historical data to make predictions about future outcomes. It involves statistical models and machine learning algorithms to forecast trends and probabilities.\n",
        "  Tools/Techniques: Machine learning (regression, classification), forecasting models, data mining, statistical modeling.\n",
        "  Value: Enables organizations to anticipate future events, assess risks, and identify opportunities (e.g., \"What will our sales be next quarter? Which customers are likely to churn?\").\n",
        "\n",
        "  Prescriptive Analytics (What should we do?):\n",
        "  Focus: Builds on predictive analytics by recommending specific actions to achieve desired outcomes or mitigate risks. It provides actionable advice.\n",
        "  Tools/Techniques: Optimization, simulation, decision trees, graph analysis, recommendation engines.\n",
        "  Value: Guides decision-making by suggesting the best course of action given various scenarios and constraints (e.g., \"What marketing strategy should we implement to maximize sales next quarter? What is the optimal pricing for our product?\").\n",
        "\n",
        "  Generative AI (What can be created or designed?):\n",
        "  Focus: This is a more recent and advanced stage, where AI models are used not just to analyze or predict, but to create new content, designs, solutions, or even code. It's about generating novel outputs based on learned patterns from vast datasets.\n",
        "  Tools/Techniques: Large Language Models (LLMs), Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), transformers.\n",
        "  Value: Revolutionizes creative processes, automates content generation, accelerates research and development, and enables personalized experiences at an unprecedented scale (e.g., \"Generate a marketing campaign for a new product, write code to automate a task, design new drug molecules, create realistic images from text descriptions.\")\n",
        "Each stage represents a progression in the sophistication of data utilization, moving from understanding the past to influencing the future and ultimately creating it."
      ],
      "metadata": {
        "id": "Zi6HgnsteCTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "p5dE9VQyt9-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H1aEYmdGGV0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N2_cu34qMuPQ"
      }
    }
  ]
}